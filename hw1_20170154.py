# -*- coding: utf-8 -*-
"""HW1_20170154

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cwjSabQJ2WJG718uzb2fbufEAvEJhiix

# [CS376] Homework 1. Linear Regression

- TA : Seokin Seo (siseo@ai.kaist.ac.kr), Youngsoo Jang (ysjang@ai.kaist.ac.kr)
- Due Date : 27th Nov, 23:59 (**Any late submissions will not be accepted.**)
- Office Hour : E3-1 2443, Tues 18:30~21:30 
- Skeleton codes for this homework are based on tutorial code for textbook written by Marc Deisenroth. 
- If you're not familar with Google colab or Jupyter notebook, contact TAs in e-mail or come TA office hour on tuesday to get our help.

## README

This notebook file is *incomplete* at the first time, so you should implement some code cells for this homework.

In addition, you should write answers for given questions.

In HW1, you should submit following file:
- `hw1.ipynb`: a **completed** version of this IPython notebook. 
  - You should **implement total 17 python code cells (which are marked as TODO)** and **write total 8 answers** for given questions. 
  - Every code cell should be working and leave outputs when you submit HW.
  - **Do NOT clear** your outputs (ex. plots, outputs, etc.).  

After finishing the homework, please submit this file to **Homework 1 section in KLMS**.



### Instructions
- First of all, you should copy the original Colab notebook into your Google drive.
  - Sign in Google.
  - Click `File` in the left top on menu and select `Save a copy in Drive..`.
- Then you can edit your copied notebook in Google Colab!
  - Fill in the `TODO` marked part to run appropriately. 
  - Each code cell will be executed when you click the play button left side of the cell (or press `Ctrl+Enter`).
- Download your copied notebook file (`.ipynb` file) to your local and rename it as `hw1.ipynb`.
- Upload the file (`hw1.ipynb`) to KLMS Homework 1 section.

**[IMPORTANT]**

Please submit the notebook file (without compressing) with the name like below:

HW1_StudentID.ipynb

ex) HW1_20191234.ipynb


### Q&A Policies
- If you think your question is important for every student, make your question about HW as a comment on the code cell block in [the original notebook](https://colab.research.google.com/drive/1Ye7fnZ3PZl2REidUhWT0aNWGDYzEPVJb), not your copy.
- If you want to ask some questions about your code, contact TAs in e-mail or come TA office hour.



---

## 0. Prerequisites

The purpose of this notebook is to practice for implementing some linear algebra (equations provided) and to explore some properties of linear regression.

You can use following packages in this HW:
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import scipy.linalg
import matplotlib.pyplot as plt
# %matplotlib inline

"""We consider a linear regression problem of the form
$$
y = \boldsymbol x^T\boldsymbol\theta + \epsilon\,,\quad \epsilon \sim \mathcal N(0, \sigma^2)
$$
where $\boldsymbol x\in\mathbb{R}^D$ are inputs and $y\in\mathbb{R}$ are noisy observations. The parameter vector $\boldsymbol\theta\in\mathbb{R}^D$ parametrizes the function.

We assume we have a training set $(\boldsymbol x_n, y_n)$, $n=1,\ldots, N$. We summarize the sets of training inputs in $\mathcal X = \{\boldsymbol x_1, \ldots, \boldsymbol x_N\}$ and corresponding training targets $\mathcal Y = \{y_1, \ldots, y_N\}$, respectively.

In this tutorial, we are interested in finding good parameters $\boldsymbol\theta$.

## 1. Maximum Likelihood
We will start with maximum likelihood estimation of the parameters $\boldsymbol\theta$. In maximum likelihood estimation, we find the parameters $\boldsymbol\theta^{\mathrm{ML}}$ that maximize the likelihood
$$
p(\mathcal Y | \mathcal X, \boldsymbol\theta) = \prod_{n=1}^N p(y_n | \boldsymbol x_n, \boldsymbol\theta)\,.
$$
From the lecture we know that the maximum likelihood estimator is given by
$$
\boldsymbol\theta^{\text{ML}} = (\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y\in\mathbb{R}^D\,,
$$
where 
$$
\boldsymbol X = [\boldsymbol x_1, \ldots, \boldsymbol x_N]^T\in\mathbb{R}^{N\times D}\,,\quad \boldsymbol y = [y_1, \ldots, y_N]^T \in\mathbb{R}^N\,.
$$
"""

# Define training set
X = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1
y = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector

# Plot the training set
plt.figure()
plt.plot(X, y, '+', markersize=10)
plt.xlabel("$x$")
plt.ylabel("$y$");

"""Let us compute the maximum likelihood estimate for a given training set

Now, make a prediction using the maximum likelihood estimate that we just found :
"""

## TODO 1: EDIT THIS FUNCTION
def max_lik_estimate(X, y):    
    # X: N x D matrix of training inputs
    # y: N x 1 vector of training targets/observations
    # returns: maximum likelihood parameters (D x 1)
    
    N, D = X.shape
    X_t=X.transpose()
    X_in=scipy.linalg.inv(np.matmul(X_t,X))
    tmp=np.matmul(X_in,X_t)
    theta_ml = np.matmul(tmp,y) ## <-- EDIT THIS LINE    
    #raise NotImplementedError

    return theta_ml

# get maximum likelihood estimate
theta_ml = max_lik_estimate(X,y)

"""Now, let's see whether we got something useful:"""

## TODO 2: EDIT THIS FUNCTION
def predict_with_estimate(Xtest, theta):
    
    # Xtest: K x D matrix of test inputs
    # theta: D x 1 vector of parameters
    # returns: prediction of f(Xtest); K x 1 vector
    
    prediction = np.matmul(Xtest,theta) ## <-- EDIT THIS LINE       
    #raise NotImplementedError
    
    return prediction

# define a test set
Xtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs

# predict the function values at the test points using the maximum likelihood estimator
ml_prediction = predict_with_estimate(Xtest, theta_ml)

# plot
plt.figure()
plt.plot(X, y, '+', markersize=10)
plt.plot(Xtest, ml_prediction)
plt.xlabel("$x$")
plt.ylabel("$y$");

"""**Questions 1**

Q1-1. Does the solution above look reasonable?

Q1-2. Play around with different values of $\theta$. How do the corresponding functions change?

Q1-3. Modify the training targets $\mathcal Y$ and re-run your computation. What changes?

**Answers 1**

Q1-1. Yes. It fits well 

Q1-2. When the data doesnt seems linear, it doesn't fit well

Q1-3. slope changes


---

Let us now look at a different training set, where we add 2.0 to every $y$-value, and compute the maximum likelihood estimate
"""

ynew = y + 2.0

plt.figure()
plt.plot(X, ynew, '+', markersize=10)
plt.xlabel("$x$")
plt.ylabel("$y$");

# get maximum likelihood estimate
theta_ml = max_lik_estimate(X, ynew)
print(theta_ml)

# define a test set
Xtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs

# predict the function values at the test points using the maximum likelihood estimator
ml_prediction = predict_with_estimate(Xtest, theta_ml)

# plot
plt.figure()
plt.plot(X, ynew, '+', markersize=10)
plt.plot(Xtest, ml_prediction)
plt.xlabel("$x$")
plt.ylabel("$y$");

"""**Questions 2**

Q2-1. This maximum likelihood estimate doesn't look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?

Q2-2. How can we fix this problem?

**Answers 2**

Q2-1. because origianl maximum likelihood estimate can't consider part of Constant, so we should implement mle which consider the Constant part

Q2-2. If we use augmented vector model for theta_ml we can fix this problem


---

Let us now define a linear regression model that is slightly more flexible:
$$
y = \theta_0 + \boldsymbol x^T \boldsymbol\theta_1 + \epsilon\,,\quad \epsilon\sim\mathcal N(0,\sigma^2)
$$
Here, we added an offset (bias) parameter $\theta_0$ to our original model.

If we now define the inputs to be the augmented vector $\boldsymbol x_{\text{aug}} = \begin{bmatrix}1\\\boldsymbol x\end{bmatrix}$, we can write the new linear regression model as 
$$
y = \boldsymbol x_{\text{aug}}^T\boldsymbol\theta_{\text{aug}} + \epsilon\,,\quad \boldsymbol\theta_{\text{aug}} = \begin{bmatrix}
\theta_0\\
\boldsymbol\theta_1
\end{bmatrix}\,.
$$
"""

N, D = X.shape
X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)

theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1

"""Let us now compute the maximum likelihood estimator for this setting.
_Hint:_ If possible, re-use code that you have already written
"""

## TODO 3: EDIT THIS FUNCTION
def max_lik_estimate_aug(X_aug, y):
  
    N, D = X_aug.shape
    X_t=X_aug.transpose()
    X_in=scipy.linalg.inv(np.matmul(X_t,X_aug))
    tmp=np.matmul(X_in,X_t)
    theta_aug_ml = np.matmul(tmp,y) ## <-- EDIT THIS LINE    
    #raise NotImplementedError
    
    return theta_aug_ml

theta_aug_ml = max_lik_estimate_aug(X_aug, ynew)

"""Now, we can make predictions again:"""

# define a test set (we also need to augment the test inputs with ones)
Xtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs

# predict the function values at the test points using the maximum likelihood estimator
ml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)

# plot
plt.figure()
plt.plot(X, ynew, '+', markersize=10)
plt.plot(Xtest, ml_prediction)
plt.xlabel("$x$")
plt.ylabel("$y$");

"""It seems this has solved our problem! 

**Questions 3**

Q3-1. Play around with the first parameter of $\boldsymbol\theta_{\text{aug}}$ and describe how the fit of the function changes.

Q3-2. Play around with the second parameter of $\boldsymbol\theta_{\text{aug}}$ and describe how the fit of the function changes.

**Answers 3**

Q3-1. the linear line moves to right when x_aug increasing

Q3-2. if y_new smaller than y graph moves below, else if y_new larger than y graph moves above


---

### Nonlinear Features : Polynomial Regression
So far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs $\boldsymbol x$, as long as the parameters $\boldsymbol\theta$ appear linearly. This means, we can learn functions of the form
$$
f(\boldsymbol x, \boldsymbol\theta) = \sum_{k = 1}^K \theta_k \phi_k(\boldsymbol x)\,,
$$
where the features $\phi_k(\boldsymbol x)$ are (possibly nonlinear) transformations of the inputs $\boldsymbol x$.

Let us have a look at an example where the observations clearly do not lie on a straight line:
"""

y = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)
plt.figure()
plt.plot(X, y, '+')
plt.xlabel("$x$")
plt.ylabel("$y$");

"""One class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree $K$ as
$$
\sum_{k=0}^K \theta_k x^k = \boldsymbol \phi(x)^T\boldsymbol\theta\,,\quad
\boldsymbol\phi(x)= 
\begin{bmatrix}
x^0\\
x^1\\
\vdots\\
x^K
\end{bmatrix}\in\mathbb{R}^{K+1}\,.
$$
Here, $\boldsymbol\phi(x)$ is a nonlinear feature transformation of the inputs $x\in\mathbb{R}$.

Similar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs:
$$
\boldsymbol\Phi = \begin{bmatrix}
\boldsymbol\phi(x_1) & \boldsymbol\phi(x_2) & \cdots & \boldsymbol\phi(x_n)
\end{bmatrix}^T \in\mathbb{R}^{N\times K+1}
$$

Let us start by computing the feature matrix $\boldsymbol \Phi$
"""

## TODO 4: EDIT THIS FUNCTION
def poly_features(X, K):
    
    # X: inputs of size N x 1
    # K: degree of the polynomial
    # computes the feature matrix Phi (N x (K+1))
    
    X = X.flatten()
    N = X.shape[0]
    
    #initialize Phi
    Phi = np.zeros((N, K+1))
    
    # Compute the feature matrix in stages
    for i in range(N):
      for j in range(K+1):
          Phi[i,j]=X[i]**j
    #Phi = np.zeros((N, K+1)) ## <-- EDIT THIS LINE    
    
    #raise NotImplementedError

    return Phi

"""With this feature matrix we get the maximum likelihood estimator as
$$
\boldsymbol \theta^\text{ML} = (\boldsymbol\Phi^T\boldsymbol\Phi)^{-1}\boldsymbol\Phi^T\boldsymbol y
$$
For reasons of numerical stability, we often add a small diagonal "jitter" $\kappa>0$ to $\boldsymbol\Phi^T\boldsymbol\Phi$ so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes
$$
\boldsymbol \theta^\text{ML} = (\boldsymbol\Phi^T\boldsymbol\Phi + \kappa\boldsymbol I)^{-1}\boldsymbol\Phi^T\boldsymbol y
$$
"""

## TODO 5: EDIT THIS FUNCTION
def nonlinear_features_maximum_likelihood(Phi, y):
    # Phi: features matrix for training inputs. Size of N x D
    # y: training targets. Size of N by 1
    # returns: maximum likelihood estimator theta_ml. Size of D x 1
    
    kappa = 1e-08 # 'jitter' term; good for numerical stability
    
    D = Phi.shape[1]  
    
    # maximum likelihood estimate
    Phi_t=Phi.transpose()
    Phi_in=scipy.linalg.inv(np.matmul(Phi_t,Phi)+kappa*np.identity(D))
    tmp=np.matmul(Phi_in,Phi_t)    
    theta_ml = np.matmul(tmp,y) ## <-- EDIT THIS LINE    

    #raise NotImplementedError
 
    return theta_ml

"""Now we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let's see how this works.

To make predictions at test inputs $\boldsymbol X_{\text{test}}\in\mathbb{R}$, we need to compute the features (nonlinear transformations) $\boldsymbol\Phi_{\text{test}}= \boldsymbol\phi(\boldsymbol X_{\text{test}})$ of $\boldsymbol X_{\text{test}}$ to give us the predicted mean
$$
\mathbb{E}[\boldsymbol y_{\text{test}}] = \boldsymbol \Phi_{\text{test}}\boldsymbol\theta^{\text{ML}}
$$
"""

K = 5 # Define the degree of the polynomial we wish to fit
Phi = poly_features(X, K) # N x (K+1) feature matrix

theta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator

# test inputs
Xtest = np.linspace(-4,4,100).reshape(-1,1)

# feature matrix for test inputs
Phi_test = poly_features(Xtest, K)

y_pred = Phi_test @ theta_ml # predicted y-values

#plt.figure()
fig=plt.figure()
ax = fig.add_subplot(1,1,1)
 

ax.set_ylim([-20, 20])
plt.plot(X, y, '+')
plt.plot(Xtest, y_pred)
plt.xlabel("$x$")
plt.ylabel("$y$");

"""Experiment with different polynomial degrees in the code above.

**Questions 4**

Q4-1. What do you observe?

Q4-2. What is a good fit?

**Answers 4**

Q4-1. nonlinear curve which pass throught the points

Q4-2. There can be many cases in which data don't follow the characteristics of linear line, so it fits well at many daily situations of data


---

### Evaluating the Quality of the Model

Let us have a look at a more interesting data set
"""

def f(x):   
    return np.cos(x) + 0.2 * np.random.normal(size=(x.shape))

X = np.linspace(-4,4,20).reshape(-1,1)
y = f(X)

plt.figure()
plt.plot(X, y, '+')
plt.xlabel("$x$")
plt.ylabel("$y$");

"""Now, let us use the work from above and fit polynomials to this dataset."""

## TODO 6: EDIT THIS CELL
K =2  # Define the degree of the polynomial we wish to fit

Phi = poly_features(X, K) # N x (K+1) feature matrix

theta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator

# test inputs
Xtest = np.linspace(-5,5,100).reshape(-1,1)
ytest = f(Xtest) # ground-truth y-values

# feature matrix for test inputs
Phi_test = poly_features(Xtest, K)

y_pred = Phi_test @ theta_ml # <-- EDIT THIS LINE

#raise NotImplementedError

# plot
plt.figure()
plt.plot(X, y, '+')
plt.plot(Xtest, y_pred)
plt.plot(Xtest, ytest)
plt.legend(["data", "prediction", "ground truth observations"])
plt.xlabel("$x$")
plt.ylabel("$y$");

"""**Question 5**

Q5-1. Try out different degrees of polynomials. Based on visual inspection, what looks like the best fit? Why?

**Answers 5**

Q5-1. when K=20, it looks like fit best. because there are twenty points and 20th polynomial equation, it fits best.(20th polynomial has 20 solutions


---

Let us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the $y$-values predicted by our polynomial and the ground-truth $y$-values. The RMSE is then defined as
$$
\text{RMSE} = \sqrt{\frac{1}{N}\sum_{n=1}^N(y_n - y_n^\text{pred})^2}
$$
Write a function that computes the RMSE.
"""

## TODO 7: EDIT THIS FUNCTION
def RMSE(y, ypred):
    tmp=0
    for i in range(y.size):
      tmp+=(y[i]-ypred[i])**2
    rmse =  (tmp/y.size)**0.5## <-- EDIT THIS LINE        
    #raise NotImplementedError
    
    return rmse

RMSE(np.array([2,1]),np.array([4,1]))

"""Now compute the RMSE for different degrees of the polynomial we want to fit."""

## TODO 8: EDIT THIS CELL
K_max = 20
rmse_train = np.zeros((K_max+1,))

for k in range(K_max+1):   
    Phi = poly_features(X, k) # N x (K+1) feature matrix
    theta_ml = nonlinear_features_maximum_likelihood(Phi, y) 
    y_pred = Phi @ theta_ml
    rmse_train[k] = RMSE(y,y_pred) # <-- EDIT THIS LINE
print(rmse_train)
#raise NotImplementedError

plt.figure()
plt.plot(rmse_train)
plt.xlabel("degree of polynomial")
plt.ylabel("RMSE");

"""**Question 6**

Q6-1. What is the best polynomial fit according to this plot? Why?

Q6-2. Write some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?

**Answers 6**

Q6-1. In my case, WHen K=18. but it can be changed. value of rmse is the lowest.

Q6-2. graph pass through the points and it has lowest value of error


---
"""

# TODO 9: WRITE THE PLOTTING CODE HERE
plt.figure()
M=min(rmse_train)
print(M)
idx=np.where(rmse_train==M)[0][0]
print(idx)

K = idx # Define the degree of the polynomial we wish to fit

Phi = poly_features(X, K) # N x (K+1) feature matrix

theta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator


# feature matrix for test inputs
Phi_test = poly_features(Xtest, K)

ypred_test =Phi_test @ theta_ml ## <--- EDIT THIS LINE (hint: you may require a few lines to do the computation)
#raise NotImplementedError
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
 

ax.set_ylim([-3, 3])

plt.plot(X, y, '+')
plt.plot(Xtest, ypred_test) 
plt.xlabel("$x$")
plt.ylabel("$y$")
plt.legend(["train_data", "maximum likelihood fit"]);

"""The RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree."""

## TODO 10: EDIT THIS CELL
K_max = 20
rmse_train = np.zeros((K_max+1,))
rmse_test = np.zeros((K_max+1,))

for k in range(K_max+1):
    
    # feature matrix
    Phi = poly_features(X, k) ## <--- EDIT THIS LINE
    #print(Phi)
    # maximum likelihood estimate
    theta_ml = nonlinear_features_maximum_likelihood(Phi, y) ## <--- EDIT THIS LINE
    
    # predict y-values of training set
    ypred_train = Phi @ theta_ml ## <--- EDIT THIS LINE
    
    # RMSE on training set
    rmse_train[k] = RMSE(y,ypred_train) ## <--- EDIT THIS LINE
            
    # feature matrix for test inputs
    Phi_test = poly_features(Xtest, k) ## <--- EDIT THIS LINE
    
    # prediction (test set)
    ypred_test = Phi_test @ theta_ml ## <--- EDIT THIS LINE
    
    # RMSE on test set
    rmse_test[k] = RMSE(ytest,ypred_test) ## <--- EDIT THIS LINE
print(rmse_test)
print(min(rmse_test))
#raise NotImplementedError

plt.figure()
plt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale
plt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale
plt.xlabel("degree of polynomial")
plt.ylabel("RMSE")
plt.legend(["training set", "test set"]);

"""**Question 7**

Q7-1. What do you observe now?

Q7-2. Why does the RMSE for the test set not always go down?

Q7-3. Which polynomial degree would you choose now?

Q7-4. Plot the fit for the "best" polynomial degree.

Q7-5. If you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?

**Answers 7**

Q7-1. value of rmse of test goes up according to degree going up

Q7-2. I think it because of overfitting

Q7-3. In my case, I choose 8

Q7-4. *(IMPLEMENT CODE CELL BELOW (TODO 11))*

Q7-5. divide the traing set into two groups and use smaller group as test set
"""

# TODO 11: WRITE THE PLOTTING CODE HERE
plt.figure()
plt.plot(X, y, '+')
idx=np.where(rmse_test==min(rmse_test))[0][0]
Phi=poly_features(X,idx)
theta_ml=nonlinear_features_maximum_likelihood(Phi,y)
Phi_test=poly_features(Xtest,idx)
ypred_test = Phi_test @ theta_ml ## <--- EDIT THIS LINE (hint: you may require a few lines to do the computation)
#raise NotImplementedError

plt.plot(Xtest, ypred_test) 
plt.xlabel("$x$")
plt.ylabel("$y$")
plt.legend(["data", "maximum likelihood fit"]);

"""---

## 2. Maximum A Posteriori Estimation

We are still considering the model
$$
y = \boldsymbol\phi(\boldsymbol x)^T\boldsymbol\theta + \epsilon\,,\quad \epsilon\sim\mathcal N(0,\sigma^2)\,.
$$
We assume that the noise variance $\sigma^2$ is known.

Instead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters $\boldsymbol\theta$, which is given as
$$
p(\boldsymbol\theta|\mathcal X, \mathcal Y) = \frac{ 
  p(\mathcal Y|\mathcal X, \boldsymbol\theta)
  p(\boldsymbol\theta)
  }{    
    p(\mathcal Y|\mathcal X)
  }
$$
The purpose of the parameter prior $p(\boldsymbol\theta)$ is to discourage the parameters to attain extreme values, a sign that the model overfits. 

The prior allows us to specify a "reasonable" range of parameter values. 

Typically, we choose a Gaussian prior $\mathcal N(\boldsymbol 0, \alpha^2\boldsymbol I)$, centered at $\boldsymbol 0$ with variance $\alpha^2$ along each parameter dimension.

The MAP estimate of the parameters is
$$
\boldsymbol\theta^{\text{MAP}} = (\boldsymbol\Phi^T\boldsymbol\Phi + \frac{\sigma^2}{\alpha^2}\boldsymbol I)^{-1}\boldsymbol\Phi^T\boldsymbol y
$$
where $\sigma^2$ is the variance of the noise.
"""

## TODO 12: EDIT THIS FUNCTION
def map_estimate_poly(Phi, y, sigma, alpha):
    # Phi: training inputs, Size of N x D
    # y: training targets, Size of D x 1
    # sigma: standard deviation of the noise 
    # alpha: standard deviation of the prior on the parameters
    # returns: MAP estimate theta_map, Size of D x 1
    
    D = Phi.shape[1] 
    Phi_t=Phi.transpose()
    Phi_in=scipy.linalg.inv(np.matmul(Phi_t,Phi)+(sigma**2/alpha**2)*np.identity(D))
    tmp=np.matmul(Phi_in,Phi_t)
    
    theta_map = np.matmul(tmp,y) ## <-- EDIT THIS LINE    

    #raise NotImplementedError
    
    return theta_map

# define the function we wish to estimate later
def g(x, sigma):
    p = np.hstack([x**0, x**1, np.sin(x)])
    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)
    return p @ w + sigma*np.random.normal(size=x.shape)

# Generate some data
sigma = 1.0 # noise standard deviation
alpha = 1.0 # standard deviation of the parameter prior
N = 20

np.random.seed(42)

X = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)
y = g(X, sigma) # training targets

plt.figure()
plt.plot(X, y, '+')
plt.xlabel("$x$")
plt.ylabel("$y$");

# get the MAP estimate
K = 8 # polynomial degree   

# feature matrix
Phi = poly_features(X, K)

theta_map = map_estimate_poly(Phi, y, sigma, alpha)

# maximum likelihood estimate
theta_ml = nonlinear_features_maximum_likelihood(Phi, y)

Xtest = np.linspace(-5,5,100).reshape(-1,1)
ytest = g(Xtest, sigma)

Phi_test = poly_features(Xtest, K)
y_pred_map = Phi_test @ theta_map
y_pred_mle = Phi_test @ theta_ml

plt.figure()
plt.plot(X, y, '+')
plt.plot(Xtest, y_pred_map)
plt.plot(Xtest, g(Xtest, 0))
plt.plot(Xtest, y_pred_mle)

plt.legend(["data", "map prediction", "ground truth function", "maximum likelihood"]);

print(np.hstack([theta_ml, theta_map]))

"""Now, let us compute the RMSE on test data for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate."""

## TODO 13: EDIT THIS CELL

K_max = 12 # this is the maximum degree of polynomial we will consider
assert(K_max < N) # this is the latest point when we'll run into numerical problems

rmse_mle = np.zeros((K_max+1,))
rmse_map = np.zeros((K_max+1,))

for k in range(K_max+1):   
    Phi = poly_features(X, k)

    theta_map = map_estimate_poly(Phi, y, sigma, alpha)

# maximum likelihood estimate
    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)
    Phi_test = poly_features(Xtest, k)
    y_pred_map = Phi_test @ theta_map
    y_pred_mle = Phi_test @ theta_ml
    rmse_mle[k] = RMSE(ytest,y_pred_mle) ## EDIT THIS LINE : Compute the maximum likelihood estimator, compute the test-set predicitons, compute the RMSE
    rmse_map[k] = RMSE(ytest,y_pred_map) ## EDIT THIS LINE : Compute the MAP estimator, compute the test-set predicitons, compute the RMSE

#raise NotImplementedError

plt.figure()
plt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale
plt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale
plt.xlabel("degree of polynomial")
plt.ylabel("RMSE")
plt.legend(["Maximum likelihood", "MAP"])

"""**Question 8**

Q8-1. What do you observe?

Q8-2. What is the influence of the prior variance on the parameters ($\alpha^2$)? Change the parameter and describe what happens.

**Answers 8**

Q8-1. two graph rmse of ML and MAP according to degree of polynomial

Q8-2. As larger as prior variance becomes, two graph seems more similar

## 3. Bayesian Linear Regression
"""

# Test inputs
Ntest = 200
Xtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs

prior_var = 0.25 # variance of the parameter prior (alpha^2). We assume this is known.
noise_var = 1.0 # noise variance (sigma^2). We assume this is known.

pol_deg = 5 # degree of the polynomial we consider at the moment

"""Assume a parameter prior $p(\boldsymbol\theta) = \mathcal N (\boldsymbol 0, \alpha^2\boldsymbol I)$. For every test input $\boldsymbol x_*$ we obtain the 
prior mean
$$
E[f(\boldsymbol x_*)] = 0
$$
and the prior (marginal) variance (ignoring the noise contribution)
$$
V[f(\boldsymbol x_*)] = \alpha^2\boldsymbol\phi(\boldsymbol x_*) \boldsymbol\phi(\boldsymbol x_*)^\top
$$
where $\boldsymbol\phi(\cdot)$ is the feature map.
"""

## TODO 14: EDIT THIS CELL

# compute the feature matrix for the test inputs
Phi_test = poly_features(Xtest, pol_deg)  # N x (pol_deg+1) feature matrix <--- EDIT THIS LINE
#raise NotImplementedError

# compute the (marginal) prior at the test input locations
# prior mean
prior_mean = np.zeros((Ntest,1))  # prior mean at test inputs (size: (Ntest,1)) <-- EDIT THIS LINE
#raise NotImplementedError

# prior variance
full_covariance = prior_var * (Phi_test @ Phi_test.transpose()) # N x N covariance matrix of all function values <-- EDIT THIS LINE
prior_marginal_var = np.diagonal(full_covariance) # marginal of size (N, )
#raise NotImplementedError

# Let us visualize the prior over functions
plt.figure()
plt.plot(Xtest, prior_mean, color="k")

conf_bound1 = np.sqrt(prior_marginal_var).flatten()
conf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()
conf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()
plt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, 
             prior_mean.flatten() - conf_bound1, alpha = 0.1, color="k")
plt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, 
                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color="k")
plt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, 
                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color="k")

plt.xlabel('$x$')
plt.ylabel('$y$')
plt.title("Prior over functions");

"""Now, we will use this prior distribution and sample functions from it."""

## TODO 15: EDIT THIS CELL

# samples from the prior
num_samples = 10

# We first need to generate random weights theta_i, which we sample from the parameter prior
random_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))

# Now, we compute the induced random functions, evaluated at the test input locations
# Every function sample is given as f_i = Phi * theta_i, 
# where theta_i is a sample from the parameter prior

sample_function = Phi_test @ random_weights # <-- EDIT THIS LINE
#raise NotImplementedError

plt.figure()
plt.plot(Xtest, sample_function, color="r")
plt.title("Plausible functions under the prior")
print("Every sampled function is a polynomial of degree "+str(pol_deg));

"""Now we are given some training inputs $\boldsymbol x_1, \dotsc, \boldsymbol x_N$, which we collect in a matrix $\boldsymbol X = [\boldsymbol x_1, \dotsc, \boldsymbol x_N]^\top\in\mathbb{R}^{N\times D}$"""

N = 10
X = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1
y = g(X, np.sqrt(noise_var)) # training targets, size Nx1

"""Now, let us compute the posterior"""

## TODO 16: EDIT THIS FUNCTION
def polyfit(X, y, K, prior_var, noise_var):
    # X: training inputs, size N x D
    # y: training targets, size N x 1
    # K: degree of polynomial we consider
    # prior_var: prior variance of the parameter distribution
    # sigma: noise variance    
    jitter = 1e-08 # increases numerical stability
    N = X.shape[0]
    
    Phi = poly_features(X, K) # N x (K+1) feature matrix     
    
    # Compute maximum likelihood estimate
    theta_ml = nonlinear_features_maximum_likelihood(Phi,y) # <-- EDIT THIS LINE         
    
    # MAP estimate
    theta_map = map_estimate_poly(Phi,y,noise_var**0.5,prior_var**0.5)# <-- EDIT THIS LINE     

    # Parameter posterior
    SN = scipy.linalg.inv(scipy.linalg.inv(prior_var * np.identity(K+1)) + jitter*np.identity(K+1) + (1/noise_var)*(Phi.transpose() @ Phi)) # covariance matrix of the parameter posterior # <-- EDIT THIS LINE     
    mN = SN @ ((1/noise_var) * (Phi.transpose() @ y)) # mean vector of the parameter posterior   # <-- EDIT THIS LINE     
    
    return (theta_ml, theta_map, mN, SN)

theta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)

"""Now, let's make predictions (ignoring the measurement noise).
 
We obtain three predictors:

\begin{align}
&\text{Maximum likelihood: }E[f(\boldsymbol X_{\text{test}})] = \boldsymbol \phi(X_{\text{test}})\boldsymbol \theta_{ml}\\
&\text{Maximum a posteriori: } E[f(\boldsymbol X_{\text{test}})] = \boldsymbol \phi(X_{\text{test}})\boldsymbol \theta_{map}\\
&\text{Bayesian: } p(f(\boldsymbol X_{\text{test}})) = \mathcal N(f(\boldsymbol X_{\text{test}}) \,|\, \boldsymbol \phi(X_{\text{test}}) \boldsymbol\theta_{\text{mean}},\, \boldsymbol\phi(X_{\text{test}}) \boldsymbol\theta_{\text{var}}  \boldsymbol\phi(X_{\text{test}})^\top)
\end{align}

We already computed all quantities. Write some code that implements all three predictors.
"""

##  TODO 17: EDIT THIS CELL

# predictions (ignoring the measurement/observations noise)
K = 5
Phi_test = poly_features(Xtest, K)
                    
# maximum likelihood predictions (just the mean)
m_mle_test = Phi_test @ theta_ml # <-- EDIT THIS LINE

# MAP predictions (just the mean)
m_map_test = Phi_test @ theta_map # <-- EDIT THIS LINE

# predictive distribution (Bayesian linear regression)
# mean prediction
mean_blr = Phi_test @ theta_mean # <-- EDIT THIS LINE

# variance prediction
cov_blr =  Phi_test @ theta_var @ Phi_test.transpose() # <-- EDIT THIS LINE

#raise NotImplementedError

# plot the posterior
plt.figure()
plt.plot(X, y, "+")
plt.plot(Xtest, m_mle_test)
plt.plot(Xtest, m_map_test)

var_blr = np.diag(cov_blr)
conf_bound1 = np.sqrt(var_blr).flatten()
conf_bound2 = 2.0*np.sqrt(var_blr).flatten()
conf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()

plt.plot(Xtest, mean_blr)
plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, 
                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color="k")
plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, 
                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color="k")
plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, 
                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color="k")
plt.legend(["Training data", "MLE", "MAP", "BLR_mean", "BLR_var"])
plt.xlabel('$x$');
plt.ylabel('$y$');

"""## Finish!"""